{
  "cells": [
    {
      "metadata": {
        "id": "d3f8b5c16e7eb563"
      },
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 6: Dense Retrieval e Introducción a FAISS\n",
        "\n",
        "## Objetivo de la práctica\n",
        "\n",
        "Generar embeddings con sentence-transformers (SBERT, E5), e indexar documentos con FAISS"
      ],
      "id": "d3f8b5c16e7eb563"
    },
    {
      "metadata": {
        "id": "cdd69ed7fcbeef9d"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 0: Carga del Corpus\n",
        "### Actividad\n",
        "\n",
        "1. Carga el corpus 20 Newsgroups desde sklearn.datasets.fetch_20newsgroups.\n",
        "2. Limita el corpus a los primeros 2000 documentos para facilitar el procesamiento."
      ],
      "id": "cdd69ed7fcbeef9d"
    },
    {
      "metadata": {
        "id": "b00fbde6cfc88b"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroupsdocs = newsgroups.data"
      ],
      "id": "b00fbde6cfc88b"
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_limitado = newsgroupsdocs[:2000]"
      ],
      "metadata": {
        "id": "KL6jFizbJvrJ"
      },
      "id": "KL6jFizbJvrJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b9184f4b3e66e20a"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 2: Generación de Embeddings\n",
        "### Actividad\n",
        "\n",
        "1. Usa dos modelos de sentence-transformers. Puedes usar: `'all-MiniLM-L6-v2'` (SBERT), o `'intfloat/e5-base'` (E5). Cuando uses E5, antepon `\"passage: \"` a cada documento antes de codificar.\n",
        "2. Genera los vectores de embeddings para todos los documentos usando el modelo seleccionado.\n",
        "3. Guarda los embeddings en un array de NumPy para su posterior indexación."
      ],
      "id": "b9184f4b3e66e20a"
    },
    {
      "metadata": {
        "id": "525ae7515c6169d9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "model_e5 = SentenceTransformer('intfloat/e5-base')\n",
        "\n",
        "print(f\"Modelos cargados: {'all-MiniLM-L6-v2'} y {'intfloat/e5-base'}\")\n",
        "print(f\"El corpus tiene {len(corpus_limitado)} documentos.\")"
      ],
      "id": "525ae7515c6169d9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "SBERT"
      ],
      "metadata": {
        "id": "ddgwvZRtLZhB"
      },
      "id": "ddgwvZRtLZhB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar los embeddings\n",
        "embeddings_sbert = model_sbert.encode(\n",
        "    corpus_limitado,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "embeddings_sbert_np = embeddings_sbert"
      ],
      "metadata": {
        "id": "nfzRrAsTKfZ5"
      },
      "id": "nfzRrAsTKfZ5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E5"
      ],
      "metadata": {
        "id": "lKI0J36YLbb4"
      },
      "id": "lKI0J36YLbb4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Anteponer \"passage: \" a cada documento\n",
        "corpus_e5 = [f\"passage: {doc}\" for doc in corpus_limitado]\n",
        "\n",
        "# Generar los embeddings con el corpus modificado\n",
        "embeddings_e5 = model_e5.encode(\n",
        "    corpus_e5,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "# El resultado ya es un array de NumPy\n",
        "embeddings_e5_np = embeddings_e5\n"
      ],
      "metadata": {
        "id": "eKLswnT7LQ7p"
      },
      "id": "eKLswnT7LQ7p",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "40462a067ca2d379"
      },
      "cell_type": "markdown",
      "source": [
        "## Parte 3: Consulta\n",
        "### Actividad\n",
        "\n",
        "1. Escribe una consulta en lenguaje natural. Ejemplos:\n",
        "\n",
        "    * \"God, religion, and spirituality\"\n",
        "    * \"space exploration\"\n",
        "    * \"car maintenance\"\n",
        "\n",
        "2. Codifica la consulta utilizando el mismo modelo de embeddings. Cuando uses E5, antepon `\"query: \"` a la consulta.\n",
        "3. Recupera los 5 documentos más relevantes con similitud coseno.\n",
        "4. Muestra los textos de los documentos recuperados (puedes mostrar solo los primeros 500 caracteres de cada uno)."
      ],
      "id": "40462a067ca2d379"
    },
    {
      "metadata": {
        "id": "aad085806124c709"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def retrieve_top_k(query_embedding, doc_embeddings, corpus, k=5):\n",
        "\n",
        "    similarity_scores = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
        "\n",
        "    top_indices = np.argsort(similarity_scores)[::-1][:k]\n",
        "\n",
        "    results = []\n",
        "    for i in top_indices:\n",
        "        results.append({\n",
        "            'index': i,\n",
        "            'score': similarity_scores[i],\n",
        "            'text': corpus[i]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "QUERY = \"space exploration\"\n",
        "\n",
        "# SBERT\n",
        "query_embedding_sbert = model_sbert.encode(QUERY, convert_to_numpy=True).reshape(1, -1)\n",
        "\n",
        "results_sbert = retrieve_top_k(query_embedding_sbert, embeddings_sbert_np, corpus_limitado, k=5)\n",
        "\n",
        "for rank, res in enumerate(results_sbert):\n",
        "    print(f\"\\n[{rank+1}. Documento #{res['index']}] (Similitud: {res['score']:.4f})\")\n",
        "    print(\"-\" * 20)\n",
        "    print(res['text'][:500].strip() + \"...\")\n",
        "\n",
        "\n",
        "# Búsqueda con E5 ('intfloat/e5-base')\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"  RESULTADOS CON E5 ('intfloat/e5-base') (Con prefijo 'query: ')\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "QUERY_E5_PREFIXED = f\"query: {QUERY}\"\n",
        "query_embedding_e5 = model_e5.encode(QUERY_E5_PREFIXED, convert_to_numpy=True).reshape(1, -1)\n",
        "\n",
        "results_e5 = retrieve_top_k(query_embedding_e5, embeddings_e5_np, corpus_limitado, k=5)\n",
        "\n",
        "for rank, res in enumerate(results_e5):\n",
        "    print(f\"\\n[{rank+1}. Documento #{res['index']}] (Similitud: {res['score']:.4f})\")\n",
        "    print(\"-\" * 20)\n",
        "    print(res['text'][:500].strip() + \"...\")"
      ],
      "id": "aad085806124c709"
    },
    {
      "metadata": {
        "id": "2dc9e5e7815c7508"
      },
      "cell_type": "markdown",
      "source": [],
      "id": "2dc9e5e7815c7508"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}